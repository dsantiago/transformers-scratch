{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 256  # d_model\n",
    "block_size = 16\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] max_length: 6, [FR] max_length: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dessert]</td>\n",
       "      <td>[dessert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[white, rice,, cooked,, unsalted]</td>\n",
       "      <td>[riz, blanc,, cuit,, non, salé]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[cashew,, dry, roasted,, unsalted]</td>\n",
       "      <td>[noix, de, cajou,, grillée, à, sec,, non, salée]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[burger, sauce]</td>\n",
       "      <td>[sauce, burger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[horse, mackerel,, oily,, raw]</td>\n",
       "      <td>[chinchard,, gras,, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>[fruit, jelly]</td>\n",
       "      <td>[pâte, de, fruits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>[celery, stalk]</td>\n",
       "      <td>[céleri, branche]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>[agar, (seaweed),, raw]</td>\n",
       "      <td>[agar, (algue),, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>[chilli, pepper,, raw]</td>\n",
       "      <td>[piment,, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>[sorbet,, on, stick]</td>\n",
       "      <td>[sorbet,, bâtonnet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1615 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      en  \\\n",
       "0                              [dessert]   \n",
       "1      [white, rice,, cooked,, unsalted]   \n",
       "2     [cashew,, dry, roasted,, unsalted]   \n",
       "3                        [burger, sauce]   \n",
       "4         [horse, mackerel,, oily,, raw]   \n",
       "...                                  ...   \n",
       "1610                      [fruit, jelly]   \n",
       "1611                     [celery, stalk]   \n",
       "1612             [agar, (seaweed),, raw]   \n",
       "1613              [chilli, pepper,, raw]   \n",
       "1614                [sorbet,, on, stick]   \n",
       "\n",
       "                                                    fr  \n",
       "0                                            [dessert]  \n",
       "1                      [riz, blanc,, cuit,, non, salé]  \n",
       "2     [noix, de, cajou,, grillée, à, sec,, non, salée]  \n",
       "3                                      [sauce, burger]  \n",
       "4                             [chinchard,, gras,, cru]  \n",
       "...                                                ...  \n",
       "1610                                [pâte, de, fruits]  \n",
       "1611                                 [céleri, branche]  \n",
       "1612                             [agar, (algue),, cru]  \n",
       "1613                                    [piment,, cru]  \n",
       "1614                               [sorbet,, bâtonnet]  \n",
       "\n",
       "[1615 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('train.parquet')\n",
    "df = df['translation'].apply(lambda r: pd.Series([r['en'].lower(), r['fr'].lower()]))\n",
    "df.columns = 'en', 'fr'\n",
    "df = df[df.en.str.len() < 30]\n",
    "df = df.apply(lambda s: s.str.strip().str.split()).reset_index(drop=True)\n",
    "print(f\"[EN] max_length: {df.en.str.len().max()}, [FR] max_length: {df.fr.str.len().max()}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1425, 1537)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "\n",
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "PAD, UNK, SOS, EOS = specials\n",
    "vocab_en = build_vocab_from_iterator(df.en, specials=specials, max_tokens=2048)\n",
    "vocab_fr = build_vocab_from_iterator(df.fr, specials=specials, max_tokens=2048)\n",
    "vocab_en.set_default_index(vocab_en[UNK])\n",
    "vocab_fr.set_default_index(vocab_fr[UNK])\n",
    "vocab_en_size, vocab_fr_size = len(vocab_en), len(vocab_fr)\n",
    "vocab_en_size, vocab_fr_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text, vocab, add_eos=True):\n",
    "    tokenized_text = [SOS] + text + ([EOS] if add_eos else [])\n",
    "    tensor = torch.zeros(block_size).long()\n",
    "    tensor[:len(tokenized_text)] = torch.as_tensor(vocab.lookup_indices(tokenized_text))\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "tokens_en = df.en.apply(lambda x: text_to_tensor(x, vocab_en, add_eos=False)).tolist()\n",
    "tokens_en = torch.cat(tokens_en, 0)\n",
    "\n",
    "tokens_fr = df.fr.apply(lambda x: text_to_tensor(x, vocab_fr, add_eos=True)).tolist()\n",
    "tokens_fr = torch.cat(tokens_fr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1615, 16]), torch.Size([1615, 16]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_en.shape, tokens_fr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Xq, Xk, Xv, mask=None):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "\n",
    "        # B, T, C = Xq.shape\n",
    "        q = self.query(Xq) # (B,T,hs)\n",
    "        k = self.key(Xk)   # (B,T,hs)\n",
    "        v = self.value(Xv) # (B,T,hs)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "\n",
    "        if mask is not None:\n",
    "            wei = wei.masked_fill(mask, float('-inf')) # (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "#------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, n_embd):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *parts, mask=None):\n",
    "        Xq, Xk, Xv = parts if len(parts) == 3 else parts * 3\n",
    "        out = torch.cat([h(Xq, Xk, Xv, mask=mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "#------------------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(n_head, n_embd)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "#------------------------------------\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(n_head, n_embd)\n",
    "        self.mha2 = MultiHeadAttention(n_head, n_embd)\n",
    "\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, y, x, pad_mask=None, future_mask=None):\n",
    "        y = y + self.mha1(self.ln1(y), mask=future_mask)\n",
    "        x = y + self.mha2(self.ln2(y), x, x, mask=pad_mask)\n",
    "        x = x + self.ffwd(self.ln3(y))\n",
    "        return x\n",
    "\n",
    "#------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_src_size, vocab_tgt_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeding_src = nn.Embedding(vocab_src_size, n_embd, padding_idx=0)\n",
    "        self.embeding_tgt = nn.Embedding(vocab_tgt_size, n_embd, padding_idx=0)\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_tgt_size)\n",
    "\n",
    "        self.register_buffer('triu_mask', torch.tril(torch.ones(block_size, block_size)) == 0)\n",
    "        self.register_buffer('pos_enc', self._positional_encoding(n_embd, max_len=block_size))\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        pad_mask = src.eq(0).unsqueeze(1).repeat(1, block_size, 1)\n",
    "        future_mask = torch.logical_or(pad_mask, self.triu_mask)\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        src_emb = self.embeding_src(src) # (B,T,C)\n",
    "        tgt_emb = self.embeding_tgt(tgt) # (B,T,C)\n",
    "\n",
    "        x = src_emb + self.pos_enc # (B,T,C) + (T,C)\n",
    "        y = tgt_emb + self.pos_enc # (B,T,C) + (T,C)\n",
    "\n",
    "        # Encoder Blocks\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, mask=pad_mask) # (B,T,C)\n",
    "\n",
    "        # Decoder Blocks\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(y, x, pad_mask=pad_mask, future_mask=future_mask) # (B,T,C)\n",
    "\n",
    "        y = self.ln_f(y) # (B,T,C)\n",
    "        logits = self.lm_head(y) # (B,T,vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def _positional_encoding(self, d_model, max_len=1000):\n",
    "        pos = torch.arange(max_len).view(-1, 1).float()\n",
    "        pe = torch.arange(d_model // 2).repeat_interleave(2).repeat(max_len, 1).float()\n",
    "        pe[:, 0::2] = torch.sin(pos / (10000 ** (2 * pe[:, 0::2] / d_model)))\n",
    "        pe[:, 1::2] = torch.cos(pos / (10000 ** (2 * pe[:, 1::2] / d_model)))\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(tokens_en, tokens_fr)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_en_size, vocab_fr_size)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), amsgrad=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfomer_train():\n",
    "    for i in range(1, EPOCHS + 1):\n",
    "        accs = []\n",
    "        for i_batch, (src, tgt) in enumerate(train_dl, start=1):\n",
    "            out = model(src, tgt)\n",
    "\n",
    "            # The last decoder prediction is meaningless because we have no target token for it.\n",
    "            # The <sos> token in the target is also something we do not want to compute a loss for.\n",
    "            out = out[:, :-1, :]\n",
    "            tgt = tgt[:, 1:]\n",
    "            loss = criterion(out.permute(0, 2, 1), tgt)\n",
    "            acc = (out.argmax(-1) == tgt).float().mean()\n",
    "            accs.append(acc.item())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            print(f\"EPOCH: {i:>02}   BATCH: {i_batch:>02}   Acc: {acc.item():.4f}   Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        print('-' * 50)\n",
    "        print(f\"EPOCH {i:>02} Full Accuracy: {np.mean(accs):.4f}\")\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "PATH = f'weights/weight-transformer-{n_embd:>03}embd.pt'\n",
    "\n",
    "if TRAIN:\n",
    "    transfomer_train()\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "else:\n",
    "    model = Transformer(vocab_en_size, vocab_fr_size)\n",
    "    model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - <sos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 2 - <sos> mousse de <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 3 - <sos> mousse de canard <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 4 - <sos> mousse de canard <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "def  eng_to_fr(english_phrase):\n",
    "    model.eval()\n",
    "\n",
    "    phrase_en = f\"{english_phrase} {f'{PAD} ' * block_size}\".strip()\n",
    "    tokens_en = vocab_en.lookup_indices(phrase_en.split()[:block_size]) \n",
    "    tokens_en = torch.tensor(tokens_en).long()\n",
    "\n",
    "    phrase_fr = ' '.join([SOS] + [PAD] * (block_size - 1))\n",
    "    tokens_fr = vocab_fr.lookup_indices(phrase_fr.split()[:block_size]) \n",
    "    tokens_fr = torch.tensor(tokens_fr).long()\n",
    "\n",
    "    for i in range(1, block_size + 1):\n",
    "        print(f\"Iter {i} - {phrase_fr}\")\n",
    "        \n",
    "        if EOS in phrase_fr: break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tokens_fr = vocab_fr.lookup_indices(phrase_fr.split()) \n",
    "            tokens_fr = torch.tensor(tokens_fr).long()\n",
    "            logits = model(tokens_en.unsqueeze(0), tokens_fr.unsqueeze(0))\n",
    "            tokens_out = logits.squeeze().argmax(-1).tolist()\n",
    "            phrase_fr = vocab_fr.lookup_tokens([vocab_fr[SOS]] + tokens_out)\n",
    "            phrase_fr = ' '.join(phrase_fr[:block_size])\n",
    "\n",
    "eng_to_fr(\"duck mousse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - <sos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 2 - <sos> riz de <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 3 - <sos> riz blanc riz <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 4 - <sos> riz blanc <eos> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "eng_to_fr(\"white rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
