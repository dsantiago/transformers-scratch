{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 256  # d_model\n",
    "block_size = 16\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] max_length: 6, [FR] max_length: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dessert]</td>\n",
       "      <td>[dessert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[white, rice,, cooked,, unsalted]</td>\n",
       "      <td>[riz, blanc,, cuit,, non, salé]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[cashew,, dry, roasted,, unsalted]</td>\n",
       "      <td>[noix, de, cajou,, grillée, à, sec,, non, salée]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[burger, sauce]</td>\n",
       "      <td>[sauce, burger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[horse, mackerel,, oily,, raw]</td>\n",
       "      <td>[chinchard,, gras,, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>[fruit, jelly]</td>\n",
       "      <td>[pâte, de, fruits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>[celery, stalk]</td>\n",
       "      <td>[céleri, branche]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>[agar, (seaweed),, raw]</td>\n",
       "      <td>[agar, (algue),, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>[chilli, pepper,, raw]</td>\n",
       "      <td>[piment,, cru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>[sorbet,, on, stick]</td>\n",
       "      <td>[sorbet,, bâtonnet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1615 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      en  \\\n",
       "0                              [dessert]   \n",
       "1      [white, rice,, cooked,, unsalted]   \n",
       "2     [cashew,, dry, roasted,, unsalted]   \n",
       "3                        [burger, sauce]   \n",
       "4         [horse, mackerel,, oily,, raw]   \n",
       "...                                  ...   \n",
       "1610                      [fruit, jelly]   \n",
       "1611                     [celery, stalk]   \n",
       "1612             [agar, (seaweed),, raw]   \n",
       "1613              [chilli, pepper,, raw]   \n",
       "1614                [sorbet,, on, stick]   \n",
       "\n",
       "                                                    fr  \n",
       "0                                            [dessert]  \n",
       "1                      [riz, blanc,, cuit,, non, salé]  \n",
       "2     [noix, de, cajou,, grillée, à, sec,, non, salée]  \n",
       "3                                      [sauce, burger]  \n",
       "4                             [chinchard,, gras,, cru]  \n",
       "...                                                ...  \n",
       "1610                                [pâte, de, fruits]  \n",
       "1611                                 [céleri, branche]  \n",
       "1612                             [agar, (algue),, cru]  \n",
       "1613                                    [piment,, cru]  \n",
       "1614                               [sorbet,, bâtonnet]  \n",
       "\n",
       "[1615 rows x 2 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('train.parquet')\n",
    "df = df['translation'].apply(lambda r: pd.Series([r['en'].lower(), r['fr'].lower()]))\n",
    "df.columns = 'en', 'fr'\n",
    "df = df[df.en.str.len() < 30]\n",
    "df = df.apply(lambda s: s.str.strip().str.split()).reset_index(drop=True)\n",
    "print(f\"[EN] max_length: {df.en.str.len().max()}, [FR] max_length: {df.fr.str.len().max()}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1425, 1537)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "\n",
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "PAD, UNK, SOS, EOS = specials\n",
    "vocab_en = build_vocab_from_iterator(df.en, specials=specials, max_tokens=2048)\n",
    "vocab_fr = build_vocab_from_iterator(df.fr, specials=specials, max_tokens=2048)\n",
    "vocab_en.set_default_index(vocab_en[UNK])\n",
    "vocab_fr.set_default_index(vocab_fr[UNK])\n",
    "vocab_en_size, vocab_fr_size = len(vocab_en), len(vocab_fr)\n",
    "vocab_en_size, vocab_fr_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text, vocab, add_sos=True):\n",
    "    tokenized_text = [SOS] + text + [EOS] if add_sos else text\n",
    "    tensor = torch.zeros(block_size).long()\n",
    "    tensor[:len(tokenized_text)] = torch.as_tensor(vocab.lookup_indices(tokenized_text))\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "tokens_en = df.en.apply(lambda x: text_to_tensor(x, vocab_en, add_sos=False)).tolist()\n",
    "tokens_en = torch.cat(tokens_en, 0)\n",
    "\n",
    "tokens_fr = df.fr.apply(lambda x: text_to_tensor(x, vocab_fr, add_sos=True)).tolist()\n",
    "tokens_fr = torch.cat(tokens_fr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1615, 16]), torch.Size([1615, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_en.shape, tokens_fr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Xq, Xk, Xv, mask=None):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "\n",
    "        # B, T, C = Xq.shape\n",
    "        q = self.query(Xq) # (B,T,hs)\n",
    "        k = self.key(Xk)   # (B,T,hs)\n",
    "        v = self.value(Xv) # (B,T,hs)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "\n",
    "        if mask is not None:\n",
    "            wei = wei.masked_fill(mask, float('-inf')) # (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "#------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, n_embd):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *parts, mask=None):\n",
    "        Xq, Xk, Xv = parts if len(parts) == 3 else parts * 3\n",
    "        out = torch.cat([h(Xq, Xk, Xv, mask=mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "#------------------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(n_head, n_embd)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "#------------------------------------\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(n_head, n_embd)\n",
    "        self.mha2 = MultiHeadAttention(n_head, n_embd)\n",
    "\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, y, x, pad_mask=None, future_mask=None):\n",
    "        y = y + self.mha1(self.ln1(y), mask=future_mask)\n",
    "        x = y + self.mha2(self.ln2(y), x, x, mask=pad_mask)\n",
    "        x = x + self.ffwd(self.ln3(y))\n",
    "        return x\n",
    "\n",
    "#------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_src_size, vocab_tgt_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeding_src = nn.Embedding(vocab_src_size, n_embd, padding_idx=0)\n",
    "        self.embeding_tgt = nn.Embedding(vocab_tgt_size, n_embd, padding_idx=0)\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_tgt_size)\n",
    "\n",
    "        self.register_buffer('triu_mask', torch.tril(torch.ones(block_size, block_size)) == 0)\n",
    "        self.register_buffer('pos_enc', self._positional_encoding(n_embd, max_len=block_size))\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # B, T = src.shape\n",
    "        pad_mask = src.eq(0).unsqueeze(1).repeat(1, block_size, 1)\n",
    "        future_mask = torch.logical_or(pad_mask, self.triu_mask)\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        src_emb = self.embeding_src(src) # (B,T,C)\n",
    "        tgt_emb = self.embeding_tgt(tgt) # (B,T,C)\n",
    "\n",
    "        x = src_emb + self.pos_enc # (B,T,C) + (T,C)\n",
    "        y = tgt_emb + self.pos_enc # (B,T,C) + (T,C)\n",
    "\n",
    "        # Encoder Blocks\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, mask=pad_mask) # (B,T,C)\n",
    "\n",
    "        # Decoder Blocks\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(y, x, pad_mask=pad_mask, future_mask=future_mask) # (B,T,C)\n",
    "\n",
    "        y = self.ln_f(y) # (B,T,C)\n",
    "        logits = self.lm_head(y) # (B,T,vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def _positional_encoding(self, d_model, max_len=1000):\n",
    "        pos = torch.arange(max_len).view(-1, 1).float()\n",
    "        pe = torch.arange(d_model // 2).repeat_interleave(2).repeat(max_len, 1).float()\n",
    "        pe[:, 0::2] = torch.sin(pos / (10000 ** (2 * pe[:, 0::2] / d_model)))\n",
    "        pe[:, 1::2] = torch.cos(pos / (10000 ** (2 * pe[:, 1::2] / d_model)))\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(tokens_en, tokens_fr)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_en_size, vocab_fr_size)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), amsgrad=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfomer_train():\n",
    "    for i in range(1, EPOCHS + 1):\n",
    "        accs = []\n",
    "        for i_batch, (src, tgt) in enumerate(train_dl, start=1):\n",
    "            out = model(src, tgt)\n",
    "            out = out[:, :-1, :]\n",
    "            tgt = tgt[:, 1:]\n",
    "            loss = criterion(out.permute(0, 2, 1), tgt)\n",
    "            acc = (out.argmax(-1) == tgt).float().mean()\n",
    "            accs.append(acc.item())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            print(f\"EPOCH: {i:>02}   BATCH: {i_batch:>02}   Acc: {acc.item():.4f}   Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        print('-' * 50)\n",
    "        print(f\"EPOCH {i:>02} Full Accuracy: {np.mean(accs):.4f}\")\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01   BATCH: 01   Acc: 0.8214   Loss: 1.0232\n",
      "EPOCH: 01   BATCH: 02   Acc: 0.8266   Loss: 1.0424\n",
      "EPOCH: 01   BATCH: 03   Acc: 0.8333   Loss: 0.9743\n",
      "EPOCH: 01   BATCH: 04   Acc: 0.8271   Loss: 1.0023\n",
      "EPOCH: 01   BATCH: 05   Acc: 0.8302   Loss: 0.9950\n",
      "EPOCH: 01   BATCH: 06   Acc: 0.8224   Loss: 1.0599\n",
      "EPOCH: 01   BATCH: 07   Acc: 0.8234   Loss: 1.0107\n",
      "EPOCH: 01   BATCH: 08   Acc: 0.8208   Loss: 1.0203\n",
      "EPOCH: 01   BATCH: 09   Acc: 0.8203   Loss: 1.0566\n",
      "EPOCH: 01   BATCH: 10   Acc: 0.8281   Loss: 1.0114\n",
      "EPOCH: 01   BATCH: 11   Acc: 0.8375   Loss: 0.9597\n",
      "EPOCH: 01   BATCH: 12   Acc: 0.8260   Loss: 0.9834\n",
      "EPOCH: 01   BATCH: 13   Acc: 0.8245   Loss: 1.0027\n",
      "--------------------------------------------------\n",
      "EPOCH 01 Full Accuracy: 0.8263\n",
      "--------------------------------------------------\n",
      "EPOCH: 02   BATCH: 01   Acc: 0.8302   Loss: 0.9517\n",
      "EPOCH: 02   BATCH: 02   Acc: 0.8417   Loss: 0.9506\n",
      "EPOCH: 02   BATCH: 03   Acc: 0.8328   Loss: 0.9599\n",
      "EPOCH: 02   BATCH: 04   Acc: 0.8406   Loss: 0.9214\n",
      "EPOCH: 02   BATCH: 05   Acc: 0.8271   Loss: 0.9694\n",
      "EPOCH: 02   BATCH: 06   Acc: 0.8380   Loss: 0.9055\n",
      "EPOCH: 02   BATCH: 07   Acc: 0.8526   Loss: 0.8830\n",
      "EPOCH: 02   BATCH: 08   Acc: 0.8417   Loss: 0.9099\n",
      "EPOCH: 02   BATCH: 09   Acc: 0.8474   Loss: 0.8634\n",
      "EPOCH: 02   BATCH: 10   Acc: 0.8333   Loss: 0.9599\n",
      "EPOCH: 02   BATCH: 11   Acc: 0.8490   Loss: 0.8317\n",
      "EPOCH: 02   BATCH: 12   Acc: 0.8495   Loss: 0.8941\n",
      "EPOCH: 02   BATCH: 13   Acc: 0.8439   Loss: 0.8882\n",
      "--------------------------------------------------\n",
      "EPOCH 02 Full Accuracy: 0.8406\n",
      "--------------------------------------------------\n",
      "EPOCH: 03   BATCH: 01   Acc: 0.8568   Loss: 0.8223\n",
      "EPOCH: 03   BATCH: 02   Acc: 0.8562   Loss: 0.8060\n",
      "EPOCH: 03   BATCH: 03   Acc: 0.8651   Loss: 0.7800\n",
      "EPOCH: 03   BATCH: 04   Acc: 0.8505   Loss: 0.8662\n",
      "EPOCH: 03   BATCH: 05   Acc: 0.8505   Loss: 0.8365\n",
      "EPOCH: 03   BATCH: 06   Acc: 0.8568   Loss: 0.8262\n",
      "EPOCH: 03   BATCH: 07   Acc: 0.8479   Loss: 0.8325\n",
      "EPOCH: 03   BATCH: 08   Acc: 0.8578   Loss: 0.8203\n",
      "EPOCH: 03   BATCH: 09   Acc: 0.8656   Loss: 0.7970\n",
      "EPOCH: 03   BATCH: 10   Acc: 0.8589   Loss: 0.8210\n",
      "EPOCH: 03   BATCH: 11   Acc: 0.8630   Loss: 0.7904\n",
      "EPOCH: 03   BATCH: 12   Acc: 0.8568   Loss: 0.8053\n",
      "EPOCH: 03   BATCH: 13   Acc: 0.8582   Loss: 0.8206\n",
      "--------------------------------------------------\n",
      "EPOCH 03 Full Accuracy: 0.8572\n",
      "--------------------------------------------------\n",
      "EPOCH: 04   BATCH: 01   Acc: 0.8661   Loss: 0.7422\n",
      "EPOCH: 04   BATCH: 02   Acc: 0.8693   Loss: 0.7495\n",
      "EPOCH: 04   BATCH: 03   Acc: 0.8677   Loss: 0.7196\n",
      "EPOCH: 04   BATCH: 04   Acc: 0.8729   Loss: 0.7081\n",
      "EPOCH: 04   BATCH: 05   Acc: 0.8745   Loss: 0.7000\n",
      "EPOCH: 04   BATCH: 06   Acc: 0.8729   Loss: 0.7099\n",
      "EPOCH: 04   BATCH: 07   Acc: 0.8646   Loss: 0.7378\n",
      "EPOCH: 04   BATCH: 08   Acc: 0.8667   Loss: 0.7396\n",
      "EPOCH: 04   BATCH: 09   Acc: 0.8828   Loss: 0.6838\n",
      "EPOCH: 04   BATCH: 10   Acc: 0.8672   Loss: 0.7562\n",
      "EPOCH: 04   BATCH: 11   Acc: 0.8641   Loss: 0.7528\n",
      "EPOCH: 04   BATCH: 12   Acc: 0.8719   Loss: 0.7044\n",
      "EPOCH: 04   BATCH: 13   Acc: 0.8751   Loss: 0.7151\n",
      "--------------------------------------------------\n",
      "EPOCH 04 Full Accuracy: 0.8704\n",
      "--------------------------------------------------\n",
      "EPOCH: 05   BATCH: 01   Acc: 0.8781   Loss: 0.6726\n",
      "EPOCH: 05   BATCH: 02   Acc: 0.8943   Loss: 0.6116\n",
      "EPOCH: 05   BATCH: 03   Acc: 0.8771   Loss: 0.6870\n",
      "EPOCH: 05   BATCH: 04   Acc: 0.8875   Loss: 0.6373\n",
      "EPOCH: 05   BATCH: 05   Acc: 0.8891   Loss: 0.6236\n",
      "EPOCH: 05   BATCH: 06   Acc: 0.8938   Loss: 0.6092\n",
      "EPOCH: 05   BATCH: 07   Acc: 0.8953   Loss: 0.6149\n",
      "EPOCH: 05   BATCH: 08   Acc: 0.8839   Loss: 0.6261\n",
      "EPOCH: 05   BATCH: 09   Acc: 0.8813   Loss: 0.6504\n",
      "EPOCH: 05   BATCH: 10   Acc: 0.8927   Loss: 0.6483\n",
      "EPOCH: 05   BATCH: 11   Acc: 0.8885   Loss: 0.6160\n",
      "EPOCH: 05   BATCH: 12   Acc: 0.8938   Loss: 0.5932\n",
      "EPOCH: 05   BATCH: 13   Acc: 0.8878   Loss: 0.6221\n",
      "--------------------------------------------------\n",
      "EPOCH 05 Full Accuracy: 0.8879\n",
      "--------------------------------------------------\n",
      "EPOCH: 06   BATCH: 01   Acc: 0.8995   Loss: 0.5389\n",
      "EPOCH: 06   BATCH: 02   Acc: 0.8932   Loss: 0.5580\n",
      "EPOCH: 06   BATCH: 03   Acc: 0.9026   Loss: 0.5765\n",
      "EPOCH: 06   BATCH: 04   Acc: 0.9026   Loss: 0.5593\n",
      "EPOCH: 06   BATCH: 05   Acc: 0.8990   Loss: 0.5539\n",
      "EPOCH: 06   BATCH: 06   Acc: 0.8917   Loss: 0.5885\n",
      "EPOCH: 06   BATCH: 07   Acc: 0.9031   Loss: 0.5537\n",
      "EPOCH: 06   BATCH: 08   Acc: 0.9036   Loss: 0.5359\n",
      "EPOCH: 06   BATCH: 09   Acc: 0.9104   Loss: 0.5162\n",
      "EPOCH: 06   BATCH: 10   Acc: 0.9000   Loss: 0.5447\n",
      "EPOCH: 06   BATCH: 11   Acc: 0.9047   Loss: 0.5311\n",
      "EPOCH: 06   BATCH: 12   Acc: 0.9068   Loss: 0.5110\n",
      "EPOCH: 06   BATCH: 13   Acc: 0.9063   Loss: 0.5298\n",
      "--------------------------------------------------\n",
      "EPOCH 06 Full Accuracy: 0.9018\n",
      "--------------------------------------------------\n",
      "EPOCH: 07   BATCH: 01   Acc: 0.9042   Loss: 0.5040\n",
      "EPOCH: 07   BATCH: 02   Acc: 0.9187   Loss: 0.4589\n",
      "EPOCH: 07   BATCH: 03   Acc: 0.9203   Loss: 0.4764\n",
      "EPOCH: 07   BATCH: 04   Acc: 0.9182   Loss: 0.4652\n",
      "EPOCH: 07   BATCH: 05   Acc: 0.9234   Loss: 0.4775\n",
      "EPOCH: 07   BATCH: 06   Acc: 0.9156   Loss: 0.4578\n",
      "EPOCH: 07   BATCH: 07   Acc: 0.9151   Loss: 0.4843\n",
      "EPOCH: 07   BATCH: 08   Acc: 0.9266   Loss: 0.4347\n",
      "EPOCH: 07   BATCH: 09   Acc: 0.9146   Loss: 0.4611\n",
      "EPOCH: 07   BATCH: 10   Acc: 0.9193   Loss: 0.4593\n",
      "EPOCH: 07   BATCH: 11   Acc: 0.9141   Loss: 0.4756\n",
      "EPOCH: 07   BATCH: 12   Acc: 0.9203   Loss: 0.4712\n",
      "EPOCH: 07   BATCH: 13   Acc: 0.9207   Loss: 0.4321\n",
      "--------------------------------------------------\n",
      "EPOCH 07 Full Accuracy: 0.9178\n",
      "--------------------------------------------------\n",
      "EPOCH: 08   BATCH: 01   Acc: 0.9354   Loss: 0.3930\n",
      "EPOCH: 08   BATCH: 02   Acc: 0.9323   Loss: 0.4138\n",
      "EPOCH: 08   BATCH: 03   Acc: 0.9323   Loss: 0.4059\n",
      "EPOCH: 08   BATCH: 04   Acc: 0.9354   Loss: 0.3981\n",
      "EPOCH: 08   BATCH: 05   Acc: 0.9333   Loss: 0.3894\n",
      "EPOCH: 08   BATCH: 06   Acc: 0.9312   Loss: 0.3801\n",
      "EPOCH: 08   BATCH: 07   Acc: 0.9333   Loss: 0.4056\n",
      "EPOCH: 08   BATCH: 08   Acc: 0.9349   Loss: 0.3795\n",
      "EPOCH: 08   BATCH: 09   Acc: 0.9286   Loss: 0.4102\n",
      "EPOCH: 08   BATCH: 10   Acc: 0.9344   Loss: 0.3957\n",
      "EPOCH: 08   BATCH: 11   Acc: 0.9307   Loss: 0.4011\n",
      "EPOCH: 08   BATCH: 12   Acc: 0.9370   Loss: 0.3925\n",
      "EPOCH: 08   BATCH: 13   Acc: 0.9300   Loss: 0.4010\n",
      "--------------------------------------------------\n",
      "EPOCH 08 Full Accuracy: 0.9330\n",
      "--------------------------------------------------\n",
      "EPOCH: 09   BATCH: 01   Acc: 0.9385   Loss: 0.3549\n",
      "EPOCH: 09   BATCH: 02   Acc: 0.9474   Loss: 0.3351\n",
      "EPOCH: 09   BATCH: 03   Acc: 0.9495   Loss: 0.3546\n",
      "EPOCH: 09   BATCH: 04   Acc: 0.9417   Loss: 0.3533\n",
      "EPOCH: 09   BATCH: 05   Acc: 0.9469   Loss: 0.3107\n",
      "EPOCH: 09   BATCH: 06   Acc: 0.9469   Loss: 0.3224\n",
      "EPOCH: 09   BATCH: 07   Acc: 0.9500   Loss: 0.3288\n",
      "EPOCH: 09   BATCH: 08   Acc: 0.9568   Loss: 0.3234\n",
      "EPOCH: 09   BATCH: 09   Acc: 0.9474   Loss: 0.3149\n",
      "EPOCH: 09   BATCH: 10   Acc: 0.9401   Loss: 0.3482\n",
      "EPOCH: 09   BATCH: 11   Acc: 0.9479   Loss: 0.3111\n",
      "EPOCH: 09   BATCH: 12   Acc: 0.9464   Loss: 0.3365\n",
      "EPOCH: 09   BATCH: 13   Acc: 0.9468   Loss: 0.3569\n",
      "--------------------------------------------------\n",
      "EPOCH 09 Full Accuracy: 0.9466\n",
      "--------------------------------------------------\n",
      "EPOCH: 10   BATCH: 01   Acc: 0.9568   Loss: 0.2856\n",
      "EPOCH: 10   BATCH: 02   Acc: 0.9563   Loss: 0.2944\n",
      "EPOCH: 10   BATCH: 03   Acc: 0.9490   Loss: 0.3010\n",
      "EPOCH: 10   BATCH: 04   Acc: 0.9573   Loss: 0.2845\n",
      "EPOCH: 10   BATCH: 05   Acc: 0.9630   Loss: 0.2757\n",
      "EPOCH: 10   BATCH: 06   Acc: 0.9594   Loss: 0.2949\n",
      "EPOCH: 10   BATCH: 07   Acc: 0.9672   Loss: 0.2535\n",
      "EPOCH: 10   BATCH: 08   Acc: 0.9599   Loss: 0.2728\n",
      "EPOCH: 10   BATCH: 09   Acc: 0.9615   Loss: 0.2715\n",
      "EPOCH: 10   BATCH: 10   Acc: 0.9630   Loss: 0.2783\n",
      "EPOCH: 10   BATCH: 11   Acc: 0.9625   Loss: 0.2664\n",
      "EPOCH: 10   BATCH: 12   Acc: 0.9635   Loss: 0.2551\n",
      "EPOCH: 10   BATCH: 13   Acc: 0.9595   Loss: 0.2537\n",
      "--------------------------------------------------\n",
      "EPOCH 10 Full Accuracy: 0.9599\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "PATH = f'weights/weight-transformer-{n_embd:>03}embd.pt'\n",
    "\n",
    "if TRAIN:\n",
    "    transfomer_train()\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "else:\n",
    "    model = Transformer(vocab_en_size, vocab_fr_size)\n",
    "    model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - <sos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 2 - <sos> mousse de <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 3 - <sos> mousse de canard <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 4 - <sos> mousse de canard <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "def  eng_to_fr(english_phrase):\n",
    "    model.eval()\n",
    "\n",
    "    phrase_en = f\"{english_phrase} {f'{PAD} ' * block_size}\".strip()\n",
    "    tokens_en = vocab_en.lookup_indices(phrase_en.split()[:block_size]) \n",
    "    tokens_en = torch.tensor(tokens_en).long()\n",
    "\n",
    "    phrase_fr = ' '.join([SOS] + [PAD] * (block_size - 1))\n",
    "    tokens_fr = vocab_fr.lookup_indices(phrase_fr.split()[:block_size]) \n",
    "    tokens_fr = torch.tensor(tokens_fr).long()\n",
    "\n",
    "    for i in range(1, block_size + 1):\n",
    "        print(f\"Iter {i} - {phrase_fr}\")\n",
    "        \n",
    "        if EOS in phrase_fr: break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tokens_fr = vocab_fr.lookup_indices(phrase_fr.split()) \n",
    "            tokens_fr = torch.tensor(tokens_fr).long()\n",
    "            logits = model(tokens_en.unsqueeze(0), tokens_fr.unsqueeze(0))\n",
    "            tokens_out = logits.squeeze().argmax(-1).tolist()\n",
    "            phrase_fr = vocab_fr.lookup_tokens([vocab_fr[SOS]] + tokens_out)\n",
    "            phrase_fr = ' '.join(phrase_fr[:block_size])\n",
    "\n",
    "eng_to_fr(\"duck mousse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 - <sos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 2 - <sos> riz de <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 3 - <sos> riz blanc riz <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Iter 4 - <sos> riz blanc <eos> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "eng_to_fr(\"white rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
